name: REPD Java Bug Prediction Analysis

on:
  issue_comment:
    types: [created]

permissions:
  pull-requests: write
  issues: write
  contents: read

jobs:
  repd-analysis:
    runs-on: ubuntu-latest
    if: contains(github.event.comment.body, 'REPD')

    steps:
      - name: Parse REPD Command
        id: parse-command
        uses: actions/github-script@v6
        with:
          script: |
            const body = context.payload.comment?.body ?? '';
            core.info(`Full comment: ${body}`);
            // Look for an explicit PR link in the comment
            const linkMatch = body.match(/https:\/\/github\.com\/[^/]+\/[^/]+\/pull\/\d+/);
            let prLink = null;
            if (linkMatch) {
              prLink = linkMatch[0];
              core.info(`PR link provided: ${prLink}`);
            } else {
              // Allow "REPD" alone when the comment is on a PR
              const hasCmdOnly = /(^|\s)REPD\s*$/.test(body);
              if (hasCmdOnly && context.payload.issue?.pull_request) {
                const { owner, repo } = context.repo;
                const prNumber = context.issue.number;
                prLink = `https://github.com/${owner}/${repo}/pull/${prNumber}`;
                core.info(`Using current PR: ${prLink}`);
              } else {
                core.setFailed('ERROR: Invalid REPD command format or missing PR link');
                return;
              }
            }
            // Extract repo owner/name/number from the PR link
            const m = prLink.match(/^https:\/\/github\.com\/([^/]+)\/([^/]+)\/pull\/(\d+)$/);
            if (!m) {
              core.setFailed(`ERROR: Could not parse repository info from PR link: ${prLink}`);
              return;
            }
            const [, repoOwner, repoName, prNumber] = m;
            const fullRepoName = `${repoOwner}-${repoName}`;
            const repoUrl = `https://github.com/${repoOwner}/${repoName}.git`;
            core.setOutput('repo_owner', repoOwner);
            core.setOutput('repo_name', repoName);
            core.setOutput('pr_number', prNumber);
            core.setOutput('full_repo_name', fullRepoName);
            core.setOutput('pr_link', prLink);
            core.setOutput('repo_url', repoUrl)

      - name: Checkout REPD Repository
        uses: actions/checkout@v4
        with:
          path: repd-repo

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          cd repd-repo
          pip install --upgrade pip
          pip install numpy==1.26.4 scikit-learn==1.7.1 pandas tensorflow torch scipy tqdm

      - name: Set up Java
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Verify REPD Setup
        run: |
          cd repd-repo
          echo "Verifying REPD setup..."
          echo "Current directory: $(pwd)"
          echo "Repository contents:"
          ls -la
          echo "Trained models directory:"
          ls -la trained_models/ || echo "No trained_models directory found"
          echo "Semantic dataset creation directory:"
          ls -la semantic-dataset-creation/ || echo "No semantic-dataset-creation directory found"
          echo "Java files in repository:"
          find . -name "*.java" -type f | head -10

      - name: Run Analysis on Target PR
        id: analysis
        run: |
          set -e
          cd repd-repo
          echo "Running REPD analysis on ${{ steps.parse-command.outputs.pr_link }}..."
          
          # Get PR details using GitHub API
          pr_api_url="https://api.github.com/repos/${{ steps.parse-command.outputs.repo_owner }}/${{ steps.parse-command.outputs.repo_name }}/pulls/${{ steps.parse-command.outputs.pr_number }}"
          pr_info=$(curl -s -H "Accept: application/vnd.github.v3+json" "$pr_api_url")
          
          # Parse base/head SHAs using Python
          base_sha=$(printf "%s" "$pr_info" | python3 -c "import sys, json; d=json.load(sys.stdin); print(d['base']['sha'])")
          head_sha=$(printf "%s" "$pr_info" | python3 -c "import sys, json; d=json.load(sys.stdin); print(d['head']['sha'])")
          echo "Base SHA: $base_sha"
          echo "Head SHA: $head_sha"
          
          # Clone the target repository
          git clone "${{ steps.parse-command.outputs.repo_url }}" target_repo
          
          # Ensure we have the PR head commit (works for forks)
          git -C target_repo fetch origin "pull/${{ steps.parse-command.outputs.pr_number }}/head:prhead" || true
          git -C target_repo fetch --all --tags --prune
          
          # Get changed files between base..head (Java only)
          merge_base=$(git -C target_repo merge-base "$base_sha" "$head_sha")
          echo "Merge base: $merge_base"
          changed_files=$(git -C target_repo diff --name-only "$merge_base" "$head_sha" | grep -E "\.java$" || true)
          
          if [ -z "$changed_files" ]; then
            echo "No Java files changed in this PR"
            echo "comment=No Java files found in the PR changes." >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "Changed Java files:"
          printf "%s\n" "$changed_files"
          
          # Create analysis directories
          mkdir -p analysis_base
          mkdir -p analysis_head
          
          # Analyze base commit
          echo "Analyzing base commit..."
          git -C target_repo checkout "$base_sha"
          
          # Create list of files to analyze for base
          base_files=()
          for file in $changed_files; do
            fpath="target_repo/$file"
            if [ -f "$fpath" ]; then
              base_files+=("$fpath")
            fi
          done
          
          if [ ${#base_files[@]} -gt 0 ]; then
            echo "Analyzing ${#base_files[@]} files in base commit..."
            python3 bug_prediction_pipeline.py --files "${base_files[@]}" --output analysis_base --report || {
              echo "Base analysis failed, trying simple predictor..."
              python3 simple_bug_predictor.py --files "${base_files[@]}" --output analysis_base || {
                echo "Base analysis completely failed"
                mkdir -p analysis_base
                echo "Base analysis failed" > analysis_base/error.txt
              }
            }
          else
            echo "No files to analyze in base commit"
            mkdir -p analysis_base
            echo "No files found in base commit" > analysis_base/error.txt
          fi
          
          # Analyze head commit
          echo "Analyzing head commit..."
          git -C target_repo checkout "$head_sha"
          
          # Create list of files to analyze for head
          head_files=()
          for file in $changed_files; do
            fpath="target_repo/$file"
            if [ -f "$fpath" ]; then
              head_files+=("$fpath")
            fi
          done
          
          if [ ${#head_files[@]} -gt 0 ]; then
            echo "Analyzing ${#head_files[@]} files in head commit..."
            python3 bug_prediction_pipeline.py --files "${head_files[@]}" --output analysis_head --report || {
              echo "Head analysis failed, trying simple predictor..."
              python3 simple_bug_predictor.py --files "${head_files[@]}" --output analysis_head || {
                echo "Head analysis completely failed"
                mkdir -p analysis_head
                echo "Head analysis failed" > analysis_head/error.txt
              }
            }
          else
            echo "No files to analyze in head commit"
            mkdir -p analysis_head
            echo "No files found in head commit" > analysis_head/error.txt
          fi
          
          # Create comparison script
          cat > compare_predictions.py << 'EOF'
          import os
          import json
          import pandas as pd
          from pathlib import Path

          def load_predictions(results_dir):
              if not os.path.exists(results_dir):
                  return None
              
              prediction_files = list(Path(results_dir).glob("*_DA_predictions.csv"))
              if not prediction_files:
                  return None
              
              try:
                  df = pd.read_csv(prediction_files[0])
                  return df
              except Exception as e:
                  print(f"Error loading predictions from {results_dir}: {e}")
                  return None

          def format_comparison_results(base_df, head_df, file_names):
              if base_df is None or head_df is None:
                  return "Unable to load prediction data for comparison."
              
              # Ensure we have the same files in both dataframes
              if 'file_path' in base_df.columns and 'file_path' in head_df.columns:
                  file_col = 'file_path'
              elif 'file_id' in base_df.columns and 'file_id' in head_df.columns:
                  file_col = 'file_id'
              else:
                  return "Unable to match files between base and head predictions."
              
              base_df_indexed = base_df.set_index(file_col)
              head_df_indexed = head_df.set_index(file_col)
              
              common_files = set(base_df_indexed.index) & set(head_df_indexed.index)
              
              if not common_files:
                  return "No common files found for comparison."
              
              lines = [
                  "### ðŸ“Š Before/After Comparison",
                  "",
                  "| File | Before (Defect Prob) | After (Defect Prob) | Change | Status Change |",
                  "|------|---------------------|-------------------|--------|---------------|"
              ]
              
              for file_path in sorted(common_files):
                  try:
                      before_prob = base_df_indexed.loc[file_path, 'probability_defective']
                      after_prob = head_df_indexed.loc[file_path, 'probability_defective']
                      before_pred = base_df_indexed.loc[file_path, 'prediction']
                      after_pred = head_df_indexed.loc[file_path, 'prediction']
                      
                      # Calculate change
                      if before_prob > 0:
                          change_pct = ((after_prob - before_prob) / before_prob) * 100
                          if change_pct > 0:
                              change_str = f"+{change_pct:.1f}%"
                          else:
                              change_str = f"{change_pct:.1f}%"
                      else:
                          change_str = "N/A"
                      
                      # Status change
                      if before_pred == after_pred:
                          status_change = "No change"
                      elif after_pred == 1:
                          status_change = "ðŸ”´ â†’ Defective"
                      else:
                          status_change = "ðŸŸ¢ â†’ Non-defective"
                      
                      # Truncate file name if too long
                      display_name = os.path.basename(file_path)
                      if len(display_name) > 30:
                          display_name = "..." + display_name[-27:]
                      
                      lines.append(f"| `{display_name}` | {before_prob:.4f} | {after_prob:.4f} | {change_str} | {status_change} |")
                      
                  except Exception as e:
                      print(f"Error processing file {file_path}: {e}")
                      continue
              
              return "\n".join(lines)

          # Load predictions
          base_df = load_predictions("analysis_base")
          head_df = load_predictions("analysis_head")
          
          # Get file names from changed files
          with open("changed_files.txt", "w") as f:
              for file in os.environ.get("CHANGED_FILES", "").split():
                  if file.endswith(".java"):
                      f.write(f"{file}\n")
          
          file_names = []
          if os.path.exists("changed_files.txt"):
              with open("changed_files.txt", "r") as f:
                  file_names = [line.strip() for line in f if line.strip()]
          
          # Generate comparison
          comparison_result = format_comparison_results(base_df, head_df, file_names)
          print(comparison_result)
          EOF
          
          # Save changed files for the comparison script
          echo "$changed_files" > changed_files.txt
          
          # Run comparison
          echo "Running comparison predictions..."
          comparison_result=$(python3 compare_predictions.py)
          
          if [ -n "$comparison_result" ]; then
            {
              echo "comment<<EOF"
              echo "$comparison_result"
              echo ""
              echo "### ðŸ“‹ Analysis Summary:"
              echo "> This analysis compares the bug prediction probabilities for Java files before and after the changes in this PR. Higher defect probabilities indicate increased risk of bugs."
              echo ""
              echo "EOF"
            } >> $GITHUB_OUTPUT
          else
            echo "comment=Comparison prediction produced no output." >> $GITHUB_OUTPUT
          fi

      - name: Comment on PR
        if: steps.analysis.outputs.comment != ''
        uses: actions/github-script@v6
        env:
          COMMENT_BODY: "${{ steps.analysis.outputs.comment }}"
          PR_LINK: "${{ steps.parse-command.outputs.pr_link }}"
          REPO_NAME: "${{ steps.parse-command.outputs.full_repo_name }}"
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const commentBody = `## ðŸ”® REPD Java Bug Prediction Analysis
            **Target PR:** ${process.env.PR_LINK}
            **Repository:** ${process.env.REPO_NAME}
            ${process.env.COMMENT_BODY}
            *Analysis performed by REPD Bug Prediction System using Deep Autoencoder (DA) model*`;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });