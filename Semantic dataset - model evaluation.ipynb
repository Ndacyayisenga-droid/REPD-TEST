{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paf/miniconda3/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Classification models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from SmoteEnsemble import SmoteEnsemble as HSME\n",
    "#Result analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score as f1_score_func\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paf/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/paf/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/paf/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/paf/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/paf/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/paf/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/paf/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#Custom imports\n",
    "from utility import calculate_results\n",
    "#\n",
    "from REPD_Impl import REPD\n",
    "from autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"ant\",\n",
    "    #\"camel\",\n",
    "    #\"log4j\",\n",
    "    #\"poi\"\n",
    "]\n",
    "dataset_versions = {\n",
    "    \"ant\":[\"1.5\"],#,\"1.6\"\n",
    "    #\"camel\":[\"1.2\",\"1.4\"],\n",
    "    #\"log4j\":[\"1.1\",\"1.2\"]\n",
    "    #\"poi\":[\"2.0\",\"2.5\"]\n",
    "}\n",
    "feature_types = [\"da\"]#\"dbn\",,\"ca\"\n",
    "per_feature_type_count = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ant 1.5\n",
      "Dataset size 292\n",
      "Non-Defective count 260\n",
      "Defective count 32\n",
      "defective share 10.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y = {}\n",
    "X = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    for version in dataset_versions[dataset]:\n",
    "        #load lables\n",
    "        y_file_name = dataset+\"-\"+version+\"_y.npy\"\n",
    "        y = np.load(\"./data/\"+y_file_name)\n",
    "        Y[(dataset,version)] = y\n",
    "    \n",
    "        #Print basic dataset information\n",
    "        print(\"Dataset\",dataset,version)\n",
    "        print(\"Dataset size\",len(y))\n",
    "        print(\"Non-Defective count\",len(y[y!=1]))\n",
    "        print(\"Defective count\",len(y[y==1]))\n",
    "        print(\"defective share\",(round(100*len(y[y==1])/len(y),2)))\n",
    "        print()\n",
    "    \n",
    "        #load datasets\n",
    "        for feature_type in feature_types:\n",
    "            for i in range(per_feature_type_count):\n",
    "                x_file_name = dataset+\"-\"+version+\"_\"+str(i)+\"_\"+feature_type+\"_X_feat.npy\"\n",
    "                x = np.load(\"./data/features/\"+feature_type+\"/\"+x_file_name)\n",
    "                X[(dataset,version,feature_type,i)] = x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_count = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant 1.5 da 9\n",
      "Running episode  1\n",
      "Running episode  2\n",
      "Running episode  3\n",
      "Running episode  4\n",
      "Running episode  5\n",
      "Running episode  6\n",
      "Running episode  7\n",
      "Running episode  8\n",
      "Running episode  9\n",
      "Running episode  10\n",
      "Running episode  11\n",
      "Running episode  12\n",
      "Running episode  13\n",
      "Running episode  14\n",
      "Running episode  15\n",
      "Running episode  16\n",
      "Running episode  17\n",
      "Running episode  18\n",
      "Running episode  19\n",
      "Running episode  20\n",
      "Running episode  21\n",
      "Running episode  22\n",
      "Running episode  23\n",
      "Running episode  24\n",
      "Running episode  25\n",
      "Running episode  26\n",
      "Running episode  27\n",
      "Running episode  28\n",
      "Running episode  29\n",
      "Running episode  30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    for version in dataset_versions[dataset]:\n",
    "        y = Y[(dataset,version)]\n",
    "        \n",
    "        for feature_type in feature_types:\n",
    "            \n",
    "            #TODO: remove\n",
    "            #if dataset == \"ant\" and version == \"1.5\" and (feature_type == \"dbn\" ):#or feature_type==\"da\"):\n",
    "                ##continue\n",
    "                \n",
    "            for i in range(per_feature_type_count):\n",
    "                \n",
    "                #TODO: remove\n",
    "                if dataset == \"ant\" and version == \"1.5\" and feature_type == \"da\" and i!=9:\n",
    "                    continue\n",
    "                \n",
    "                x = X[(dataset,version,feature_type,i)]\n",
    "                \n",
    "                print(dataset,version,feature_type,i)\n",
    "                performance_data = []\n",
    "                \n",
    "\n",
    "                #Run all the models in the experiment\n",
    "                for experiment_episode in range(1,episode_count+1,1):\n",
    "                    try:\n",
    "                        print(\"Running episode \", experiment_episode)\n",
    "\n",
    "                        #Test train split\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "                        #ADP===========================================================================================\n",
    "                        autoencoder = AutoEncoder([100,50],0.01)\n",
    "                        classifer = REPD(autoencoder)\n",
    "                        classifer.fit(X_train,y_train)\n",
    "                        y_p = classifer.predict(X_test)\n",
    "                        matrix, accuracy, precision, recall, f1_score = calculate_results(y_test,y_p)\n",
    "\n",
    "                        #Store results\n",
    "                        data = ['REPD',accuracy, precision, recall, f1_score, dataset, version, feature_type, i]\n",
    "                        performance_data.append(data)\n",
    "\n",
    "                        #Close\n",
    "                        autoencoder.close()\n",
    "                        #GaussianNB===============================================================================================\n",
    "                        classifier = GaussianNB()\n",
    "                        classifier.fit(X_train,y_train)\n",
    "                        y_p = classifier.predict(X_test)\n",
    "                        matrix, accuracy, precision, recall, f1_score = calculate_results(y_test,y_p)\n",
    "\n",
    "                        #Store results\n",
    "                        data = ['GaussianNB',accuracy, precision, recall, f1_score, dataset, version, feature_type, i]\n",
    "                        performance_data.append(data)\n",
    "                        #LogisticRegression===========================================================================================\n",
    "                        classifier = LogisticRegression()\n",
    "                        classifier.fit(X_train,y_train)\n",
    "                        y_p = classifier.predict(X_test)\n",
    "                        matrix, accuracy, precision, recall, f1_score = calculate_results(y_test,y_p)\n",
    "\n",
    "                        #Store results\n",
    "                        data = ['LogisticRegression',accuracy, precision, recall, f1_score, dataset, version, feature_type, i]\n",
    "                        performance_data.append(data)\n",
    "                        #KNeighborsClassifier=========================================================================================\n",
    "                        classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "                        classifier.fit(X_train,y_train)\n",
    "                        y_p = classifier.predict(X_test)\n",
    "                        matrix, accuracy, precision, recall, f1_score = calculate_results(y_test,y_p)\n",
    "\n",
    "                        #Store results\n",
    "                        data = ['KNeighborsClassifier',accuracy, precision, recall, f1_score, dataset, version, feature_type, i]\n",
    "                        performance_data.append(data)\n",
    "\n",
    "                        #DecisionTreeClassifier=======================================================================================\n",
    "                        classifier = DecisionTreeClassifier()\n",
    "                        classifier.fit(X_train,y_train)\n",
    "                        y_p = classifier.predict(X_test)\n",
    "                        matrix, accuracy, precision, recall, f1_score = calculate_results(y_test,y_p)\n",
    "\n",
    "                        #Store results\n",
    "                        data = ['DecisionTreeClassifier',accuracy, precision, recall, f1_score, dataset, version, feature_type, i]\n",
    "                        performance_data.append(data)\n",
    "                        #HSME=======================================================================================\n",
    "                        classifier = HSME()\n",
    "                        classifier.fit(X_train,y_train)\n",
    "                        y_p = classifier.predict(X_test)\n",
    "                        matrix, accuracy, precision, recall, f1_score = calculate_results(y_test,y_p)\n",
    "                        accuracy = balanced_accuracy_score(y_test,y_p)\n",
    "\n",
    "                        #Store results\n",
    "                        data = ['HSME',accuracy, precision, recall, f1_score, dataset, version, feature_type, i]\n",
    "                        performance_data.append(data)                    \n",
    "                        #=============================================================================================================\n",
    "                    except:\n",
    "                        pass\n",
    "                results_df = pd.DataFrame(performance_data, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', \"Dataset\", \"Version\", \"Feature_type\", \"i\"])\n",
    "\n",
    "                with open(\"results/\"+dataset+'_'+version+'_'+feature_type+'_'+str(i), 'a') as f:\n",
    "                    results_df.to_csv(f, header=False)\n",
    "\n",
    "                print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
